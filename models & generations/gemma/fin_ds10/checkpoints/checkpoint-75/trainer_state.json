{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.2,
  "eval_steps": 500,
  "global_step": 75,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 6.877225875854492,
      "learning_rate": 0.0,
      "loss": 4.2275,
      "step": 1
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 6.618513584136963,
      "learning_rate": 4e-05,
      "loss": 4.1998,
      "step": 2
    },
    {
      "epoch": 0.008,
      "grad_norm": 5.937733173370361,
      "learning_rate": 8e-05,
      "loss": 4.1137,
      "step": 3
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 6.362602710723877,
      "learning_rate": 0.00012,
      "loss": 4.2087,
      "step": 4
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 2.9376797676086426,
      "learning_rate": 0.00016,
      "loss": 2.4846,
      "step": 5
    },
    {
      "epoch": 0.016,
      "grad_norm": 3.805459976196289,
      "learning_rate": 0.0002,
      "loss": 3.3339,
      "step": 6
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 2.5785160064697266,
      "learning_rate": 0.00019945945945945947,
      "loss": 2.948,
      "step": 7
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 1.9055341482162476,
      "learning_rate": 0.00019891891891891895,
      "loss": 2.4389,
      "step": 8
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.4031705856323242,
      "learning_rate": 0.00019837837837837838,
      "loss": 1.8098,
      "step": 9
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 1.1893669366836548,
      "learning_rate": 0.00019783783783783784,
      "loss": 2.1596,
      "step": 10
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 1.469341516494751,
      "learning_rate": 0.0001972972972972973,
      "loss": 2.3476,
      "step": 11
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.9608316421508789,
      "learning_rate": 0.00019675675675675678,
      "loss": 2.0345,
      "step": 12
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 1.323121190071106,
      "learning_rate": 0.00019621621621621622,
      "loss": 2.4135,
      "step": 13
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 1.5373728275299072,
      "learning_rate": 0.00019567567567567567,
      "loss": 2.1066,
      "step": 14
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.1571100950241089,
      "learning_rate": 0.00019513513513513516,
      "loss": 1.9718,
      "step": 15
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 1.4137201309204102,
      "learning_rate": 0.00019459459459459462,
      "loss": 1.8926,
      "step": 16
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 1.2671282291412354,
      "learning_rate": 0.00019405405405405405,
      "loss": 2.4635,
      "step": 17
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.0343527793884277,
      "learning_rate": 0.00019351351351351353,
      "loss": 1.8622,
      "step": 18
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 1.17295241355896,
      "learning_rate": 0.000192972972972973,
      "loss": 1.8716,
      "step": 19
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 1.1883050203323364,
      "learning_rate": 0.00019243243243243245,
      "loss": 1.7644,
      "step": 20
    },
    {
      "epoch": 0.056,
      "grad_norm": 1.151836633682251,
      "learning_rate": 0.0001918918918918919,
      "loss": 2.0123,
      "step": 21
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.9048742055892944,
      "learning_rate": 0.00019135135135135137,
      "loss": 1.5643,
      "step": 22
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 1.0325489044189453,
      "learning_rate": 0.00019081081081081082,
      "loss": 1.9218,
      "step": 23
    },
    {
      "epoch": 0.064,
      "grad_norm": 1.1294333934783936,
      "learning_rate": 0.00019027027027027028,
      "loss": 2.0271,
      "step": 24
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.9616460800170898,
      "learning_rate": 0.00018972972972972974,
      "loss": 1.7759,
      "step": 25
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.96408611536026,
      "learning_rate": 0.0001891891891891892,
      "loss": 1.8438,
      "step": 26
    },
    {
      "epoch": 0.072,
      "grad_norm": 1.0139658451080322,
      "learning_rate": 0.00018864864864864866,
      "loss": 1.9151,
      "step": 27
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 1.091966152191162,
      "learning_rate": 0.00018810810810810814,
      "loss": 2.0995,
      "step": 28
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 1.3933130502700806,
      "learning_rate": 0.00018756756756756757,
      "loss": 2.3897,
      "step": 29
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.003337025642395,
      "learning_rate": 0.00018702702702702703,
      "loss": 1.7614,
      "step": 30
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 1.075245976448059,
      "learning_rate": 0.0001864864864864865,
      "loss": 1.6282,
      "step": 31
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 1.9772392511367798,
      "learning_rate": 0.00018594594594594597,
      "loss": 2.48,
      "step": 32
    },
    {
      "epoch": 0.088,
      "grad_norm": 1.3449668884277344,
      "learning_rate": 0.0001854054054054054,
      "loss": 2.1226,
      "step": 33
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 1.141533613204956,
      "learning_rate": 0.00018486486486486486,
      "loss": 1.3791,
      "step": 34
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 1.4445492029190063,
      "learning_rate": 0.00018432432432432435,
      "loss": 1.9208,
      "step": 35
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.2002058029174805,
      "learning_rate": 0.0001837837837837838,
      "loss": 1.9514,
      "step": 36
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 1.7052271366119385,
      "learning_rate": 0.00018324324324324324,
      "loss": 1.9032,
      "step": 37
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 1.3440073728561401,
      "learning_rate": 0.00018270270270270272,
      "loss": 1.9558,
      "step": 38
    },
    {
      "epoch": 0.104,
      "grad_norm": 1.0813192129135132,
      "learning_rate": 0.00018216216216216218,
      "loss": 1.4169,
      "step": 39
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.4746681451797485,
      "learning_rate": 0.00018162162162162164,
      "loss": 1.4689,
      "step": 40
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 1.787958025932312,
      "learning_rate": 0.0001810810810810811,
      "loss": 2.2091,
      "step": 41
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.3649455308914185,
      "learning_rate": 0.00018054054054054055,
      "loss": 1.5033,
      "step": 42
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 1.7886615991592407,
      "learning_rate": 0.00018,
      "loss": 2.0446,
      "step": 43
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 1.4620864391326904,
      "learning_rate": 0.00017945945945945947,
      "loss": 1.4675,
      "step": 44
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.7902109622955322,
      "learning_rate": 0.00017891891891891893,
      "loss": 1.7094,
      "step": 45
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 1.869252324104309,
      "learning_rate": 0.00017837837837837839,
      "loss": 2.1261,
      "step": 46
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 1.7437894344329834,
      "learning_rate": 0.00017783783783783784,
      "loss": 1.5214,
      "step": 47
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.8513864278793335,
      "learning_rate": 0.00017729729729729733,
      "loss": 1.833,
      "step": 48
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 2.322317123413086,
      "learning_rate": 0.00017675675675675676,
      "loss": 1.9375,
      "step": 49
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.278719663619995,
      "learning_rate": 0.00017621621621621622,
      "loss": 1.7751,
      "step": 50
    },
    {
      "epoch": 0.136,
      "grad_norm": 2.3799455165863037,
      "learning_rate": 0.00017567567567567568,
      "loss": 1.939,
      "step": 51
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 2.5781452655792236,
      "learning_rate": 0.00017513513513513516,
      "loss": 2.1145,
      "step": 52
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 2.381408452987671,
      "learning_rate": 0.0001745945945945946,
      "loss": 2.1867,
      "step": 53
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.1254231929779053,
      "learning_rate": 0.00017405405405405405,
      "loss": 1.9053,
      "step": 54
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 2.0424387454986572,
      "learning_rate": 0.00017351351351351353,
      "loss": 1.621,
      "step": 55
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 3.372255802154541,
      "learning_rate": 0.000172972972972973,
      "loss": 2.0786,
      "step": 56
    },
    {
      "epoch": 0.152,
      "grad_norm": 4.307981014251709,
      "learning_rate": 0.00017243243243243242,
      "loss": 1.8918,
      "step": 57
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 2.5993664264678955,
      "learning_rate": 0.0001718918918918919,
      "loss": 1.9238,
      "step": 58
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 2.621014356613159,
      "learning_rate": 0.00017135135135135137,
      "loss": 2.0143,
      "step": 59
    },
    {
      "epoch": 0.16,
      "grad_norm": 2.276278495788574,
      "learning_rate": 0.00017081081081081083,
      "loss": 1.9947,
      "step": 60
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 2.809645652770996,
      "learning_rate": 0.00017027027027027028,
      "loss": 2.1187,
      "step": 61
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 2.7755424976348877,
      "learning_rate": 0.00016972972972972974,
      "loss": 2.1198,
      "step": 62
    },
    {
      "epoch": 0.168,
      "grad_norm": 2.7053771018981934,
      "learning_rate": 0.0001691891891891892,
      "loss": 2.1516,
      "step": 63
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 2.4289493560791016,
      "learning_rate": 0.00016864864864864866,
      "loss": 1.8443,
      "step": 64
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 2.6478617191314697,
      "learning_rate": 0.00016810810810810812,
      "loss": 2.1471,
      "step": 65
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.477837324142456,
      "learning_rate": 0.00016756756756756757,
      "loss": 1.8906,
      "step": 66
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 2.2674756050109863,
      "learning_rate": 0.00016702702702702703,
      "loss": 1.5075,
      "step": 67
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 3.3046820163726807,
      "learning_rate": 0.00016648648648648652,
      "loss": 2.4311,
      "step": 68
    },
    {
      "epoch": 0.184,
      "grad_norm": 2.825424909591675,
      "learning_rate": 0.00016594594594594595,
      "loss": 1.927,
      "step": 69
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 2.9674625396728516,
      "learning_rate": 0.0001654054054054054,
      "loss": 1.7016,
      "step": 70
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 2.8803763389587402,
      "learning_rate": 0.00016486486486486486,
      "loss": 1.8123,
      "step": 71
    },
    {
      "epoch": 0.192,
      "grad_norm": 3.2406299114227295,
      "learning_rate": 0.00016432432432432435,
      "loss": 1.9261,
      "step": 72
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 2.63456130027771,
      "learning_rate": 0.00016378378378378378,
      "loss": 1.9513,
      "step": 73
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 3.5585780143737793,
      "learning_rate": 0.00016324324324324324,
      "loss": 2.2067,
      "step": 74
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.854752540588379,
      "learning_rate": 0.00016270270270270272,
      "loss": 1.939,
      "step": 75
    }
  ],
  "logging_steps": 1,
  "max_steps": 375,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 75,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.03203881240832e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
